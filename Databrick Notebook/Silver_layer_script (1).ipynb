{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aec6500-8aba-483f-be4b-28f4ce0ea364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#ðŸ”¹ Step 1 â€” Configure Secure Access to ADLS via Service Principal (OAuth)\n",
    "\n",
    "This step configures secure access to Azure Data Lake Storage (ADLS Gen2) using a Service Principal (Client ID, Tenant ID, and Secret) stored in Azure Key Vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b61e6fc-2cdf-4b3e-abbf-525d5914598b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# attach this notebook to your cluster before running\n",
    "storage_account = \"adlsclinicalpoc\"  # exact storage account name\n",
    "\n",
    "\n",
    "# secrets from your scope  .\n",
    "client_id     = dbutils.secrets.get(\"clinical-keyvault-scope\", \"adls-client-id\")\n",
    "tenant_id     = dbutils.secrets.get(\"clinical-keyvault-scope\", \"adls-tenant-id\")\n",
    "client_secret = dbutils.secrets.get(\"clinical-keyvault-scope\", \"adls-client-secret\")\n",
    "\n",
    "# ABFS OAuth configs (session-scoped)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n",
    "               f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4afff8ad-f85b-4dff-8a66-08124e8bfd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##ðŸ”¹ Step 2 â€” Define and Validate RAW â†’ SILVER Data Paths in ADLS (ABFS)\n",
    "\n",
    "This step sets up the Azure Data Lake Storage (ADLS) paths for both RAW inputs and SILVER outputs using ABFS URIs (no mounts).\n",
    "It also performs a sanity check to confirm that your paths and permissions are valid, and previews a few rows from each dataset to verify schema and data integrity before transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba43f4c-c0e4-4432-b2ab-97bdc83a3efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # --- inputs: set your container names (adjust if needed)\n",
    "storage_account=\"adlsclinicalpoc\"\n",
    "container_bronze    = \"bronze\"\n",
    "container_silver = \"silver\"\n",
    "\n",
    "# --- ABFS paths to RAW inputs (no mounts!)\n",
    "BRONZE_SUBJECTS_TRIALS_PATH = f\"abfss://{container_bronze}@{storage_account}.dfs.core.windows.net/db_trial_subjects/\"\n",
    "BRONZE_LABS_RESULTS_PATH     = f\"abfss://{container_bronze}@{storage_account}.dfs.core.windows.net/adls_lab_results/\"\n",
    "BRONZE_API_ADVERSE_EVENTS_PATH       = f\"abfss://{container_bronze}@{storage_account}.dfs.core.windows.net/api_adverse_events/\"\n",
    "\n",
    "# --- ABFS paths to SILVER outputs\n",
    "SILVER_SUBJECTS_PATH = f\"abfss://{container_silver}@{storage_account}.dfs.core.windows.net/trial_subjects/\"\n",
    "SILVER_LABS_PATH     = f\"abfss://{container_silver}@{storage_account}.dfs.core.windows.net/lab_results/\"\n",
    "SILVER_AE_PATH       = f\"abfss://{container_silver}@{storage_account}.dfs.core.windows.net/adverse_events/\"\n",
    "\n",
    "# --- Sanity check: list RAW folders (confirms path + permissions)\n",
    "display(dbutils.fs.ls(BRONZE_SUBJECTS_TRIALS_PATH))\n",
    "display(dbutils.fs.ls(BRONZE_LABS_RESULTS_PATH))\n",
    "display(dbutils.fs.ls(BRONZE_API_ADVERSE_EVENTS_PATH))\n",
    "\n",
    "# --- Peek a few rows from each source to confirm format/columns\n",
    "subjects_sample = spark.read.option(\"header\", True).csv(BRONZE_SUBJECTS_TRIALS_PATH)\n",
    "labs_sample     = spark.read.option(\"header\", True).csv(BRONZE_LABS_RESULTS_PATH)\n",
    "ae_sample       = spark.read.json(BRONZE_API_ADVERSE_EVENTS_PATH)\n",
    "\n",
    "display(subjects_sample.limit(5))\n",
    "display(labs_sample.limit(5))\n",
    "display(ae_sample.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df89d8d2-efe4-4ece-9b70-dda89df85642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STEP 3: Set Spark session defaults for consistency and performance ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0577e5-8a89-4f26-a783-e69b3d00a6aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# All timestamps in UTC (prevents timezone drift)\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# Default shuffle partitions (controls number of output files)\n",
    "# 200 is safe for small-to-medium data; tune later for big data\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "# Case-insensitive column names (so 'Subject_ID' == 'subject_id')\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"false\")\n",
    "\n",
    "print(\"âœ… Spark session defaults set successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0679825f-5a1e-48bb-8f1d-7c15aa569ede",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760432691013}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_cfde1657\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_a4d6acda\",\"enabled\":true,\"columnId\":\"dob\",\"dataType\":\"date\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1760432695317}],\"syncTimestamp\":1760432695318}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DateType\n",
    "from pyspark.sql.functions import to_date,col, trim, upper, when, length, to_timestamp, coalesce, lit,substring\n",
    "\n",
    "# ---------- Explicit schema for RAW trial subjects ----------\n",
    "subjects_schema = StructType([\n",
    "    StructField(\"subject_id\",     StringType(), True),\n",
    "    StructField(\"trial_id\",       StringType(), True),\n",
    "    StructField(\"site_id\",        StringType(), True),\n",
    "    StructField(\"enrollment_date\",StringType(), True),  # read as string; we'll parse to timestamp\n",
    "    StructField(\"arm\",            StringType(), True),\n",
    "    StructField(\"sex\",            StringType(), True),\n",
    "    StructField(\"dob\",            StringType(), True),  # read as string; we'll parse to date/timestamp if needed later\n",
    "    StructField(\"country\",        StringType(), True)\n",
    "])\n",
    "\n",
    "# ---------- Read RAW as defined earlier (no mounts, ABFS path variable) ----------\n",
    "raw_subjects_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .schema(subjects_schema)\n",
    "         .csv(BRONZE_SUBJECTS_TRIALS_PATH)\n",
    ")\n",
    "\n",
    "print(f\"Raw rows: {raw_subjects_df.count()}\")\n",
    "\n",
    "# ---------- Normalize text columns: trim + uppercase COUNTRY only ----------\n",
    "norm_df = (\n",
    "    raw_subjects_df\n",
    "      .withColumn(\"subject_id\", trim(col(\"subject_id\")))\n",
    "      .withColumn(\"trial_id\",   trim(col(\"trial_id\")))\n",
    "      .withColumn(\"site_id\",    trim(col(\"site_id\")))\n",
    "      .withColumn(\"arm\",        trim(col(\"arm\")))\n",
    "      .withColumn(\"sex\",        trim(col(\"sex\")))\n",
    "      .withColumn(\"dob\",        trim(col(\"dob\")))\n",
    "      .withColumn(\"country\",    upper(trim(col(\"country\"))))\n",
    ")\n",
    "\n",
    "# ---------- Parse enrollment_date robustly (try multiple formats) ----------\n",
    "# Common patterns we may see: ISO Z, ISO without Z, date-only\n",
    "enroll_ts = coalesce(\n",
    "    to_timestamp(col(\"enrollment_date\"), \"dd-MM-yyyy\"),\n",
    "    to_timestamp(col(\"enrollment_date\"), \"dd/MM/yyyy\"),\n",
    "    to_timestamp(col(\"enrollment_date\"))  # very last resort\n",
    ")\n",
    "\n",
    "subjects_parsed = norm_df.withColumn(\"enrollment_ts_utc\", enroll_ts)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Filter: drop null/blank subject_id ----------\n",
    "subjects_nonnull = subjects_parsed.filter(\n",
    "    (col(\"subject_id\").isNotNull()) & (length(col(\"subject_id\")) > 0)\n",
    ")\n",
    "\n",
    "# ---------- Deduplicate on business key subject_id (keep the latest enrollment if tie) ----------\n",
    "# If you have a rule, adjust here. For now, use a simple dropDuplicates.\n",
    "silver_subjects = subjects_nonnull.dropDuplicates([\"subject_id\"])\n",
    "\n",
    "print(f\"After cleaning (non-null + dedupe) rows: {silver_subjects.count()}\")\n",
    "\n",
    "# ---------- Reorder/select final columns for Silver ----------\n",
    "silver_subjects_final = (\n",
    "    silver_subjects\n",
    "      .withColumn(\"dob\", to_date(substring(col(\"dob\"), 1, 10), \"yyyy-MM-dd\")) \n",
    "      .withColumn(\"enrollment_date\", to_date(col(\"enrollment_ts_utc\")))  # convert timestamp â†’ date\n",
    "      .drop(\"enrollment_ts_utc\")                                        # remove old timestamp column\n",
    "      .select(\n",
    "          \"subject_id\", \"trial_id\", \"site_id\",\n",
    "          \"enrollment_date\",      # new clean date column\n",
    "          \"arm\", \"sex\",\n",
    "          \"dob\",\"country\"\n",
    "      )\n",
    ")\n",
    "\n",
    "\n",
    "display(silver_subjects_final.limit(10))\n",
    "\n",
    "(\n",
    "    silver_subjects_final\n",
    "        .write\n",
    "        .mode(\"append\")      # overwrite each run (safe for derived Silver data)\n",
    "        .parquet(SILVER_SUBJECTS_PATH)  # write to ADLS Silver path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc71a646-a4f8-4b00-9bcb-cf998633c4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, to_timestamp, coalesce, length,\n",
    "    regexp_replace, when, date_format\n",
    ")\n",
    "\n",
    "# --------- INPUT / OUTPUT PATHS ----------\n",
    "# Make sure these exist in your notebook\n",
    "# BRONZE_LABS_RESULTS_PATH = f\"abfss://{container_raw}@{storage_account}.dfs.core.windows.net/adls_lab_results/\"\n",
    "SILVER_LAB_RESULTS = f\"abfss://{container_silver}@{storage_account}.dfs.core.windows.net/lab_results/\"\n",
    "\n",
    "# --------- SCHEMA ----------\n",
    "labs_results_schema = StructType([\n",
    "    StructField(\"lab_result_id\",         StringType(), True),\n",
    "    StructField(\"subject_id\",            StringType(), True),\n",
    "    StructField(\"encounter_id\",          StringType(), True),\n",
    "    StructField(\"order_datetime_utc\",    StringType(), True),\n",
    "    StructField(\"collected_datetime_utc\",StringType(), True),\n",
    "    StructField(\"result_datetime_utc\",   StringType(), True),\n",
    "    StructField(\"loinc_code\",            StringType(), True),\n",
    "    StructField(\"test_name\",             StringType(), True),\n",
    "    StructField(\"result_value\",          StringType(), True),\n",
    "    StructField(\"unit\",                  StringType(), True),\n",
    "    StructField(\"abnormal_flag\",         StringType(), True)\n",
    "])\n",
    "\n",
    "# --------- READ RAW ----------\n",
    "raw_labs_results_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .schema(labs_results_schema)\n",
    "         .csv(BRONZE_LABS_RESULTS_PATH)\n",
    ")\n",
    "\n",
    "# --------- BASIC NORMALIZATION ----------\n",
    "normalized_labs_results = (\n",
    "    raw_labs_results_df\n",
    "        .withColumn(\"lab_result_id\", trim(col(\"lab_result_id\")))\n",
    "        .withColumn(\"subject_id\",     trim(col(\"subject_id\")))\n",
    "        .withColumn(\"encounter_id\",   trim(col(\"encounter_id\")))\n",
    "        .withColumn(\"test_name\",      upper(trim(col(\"test_name\"))))\n",
    "        .withColumn(\"unit\",           upper(trim(col(\"unit\"))))\n",
    "        .withColumn(\"abnormal_flag\",  upper(trim(col(\"abnormal_flag\"))))\n",
    "        .withColumn(\"loinc_code\",     trim(col(\"loinc_code\")))\n",
    "        .withColumn(\"result_value\",   regexp_replace(col(\"result_value\"), \",\", \"\"))  # \"1,200\" -> \"1200\"\n",
    ")\n",
    "\n",
    "\n",
    "# --------- ENSURE NUMERIC + FLAG NORMALIZATION ----------\n",
    "labs_clean = (\n",
    "    labs_ts\n",
    "        .withColumn(\"result_value\", col(\"result_value\").cast(\"double\"))\n",
    "        .withColumn(\n",
    "            \"abnormal_flag\",\n",
    "            when(col(\"abnormal_flag\").isin(\"H\", \"HIGH\"), \"HIGH\")\n",
    "            .when(col(\"abnormal_flag\").isin(\"L\", \"LOW\"),  \"LOW\")\n",
    "            .when(col(\"abnormal_flag\").isin(\"N\", \"NORMAL\"), \"NORMAL\")\n",
    "            .otherwise(\"UNKNOWN\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# --------- FILTER INTEGRITY + DEDUPE ----------\n",
    "labs_filtered = (\n",
    "    labs_clean\n",
    "        .filter((col(\"lab_result_id\").isNotNull()) & (length(col(\"lab_result_id\")) > 0))\n",
    "        .filter(col(\"loinc_code\").isNotNull())\n",
    "        .dropDuplicates([\"lab_result_id\"])\n",
    ")\n",
    "\n",
    "# --------- FINAL SELECT (RECOMMENDED: keep timestamps) ----------\n",
    "\n",
    "#(Optional) if you truly want human-readable strings INSTEAD of timestamps:\n",
    "silver_labs_results_final = (\n",
    "    labs_filtered\n",
    "      .withColumn(\"order_datetime\",     date_format(col(\"order_datetime_utc\"),     \"yyyy-MM-dd HH:mm:ss\"))\n",
    "      .withColumn(\"collected_datetime\", date_format(col(\"collected_datetime_utc\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "      .withColumn(\"result_datetime\",    date_format(col(\"result_datetime_utc\"),    \"yyyy-MM-dd HH:mm:ss\"))\n",
    "      .select(\n",
    "          \"lab_result_id\",\"subject_id\",\"encounter_id\",\"loinc_code\",\"test_name\",\n",
    "          \"result_value\",\"unit\",\"abnormal_flag\",\n",
    "          \"order_datetime\",\"collected_datetime\",\"result_datetime\"\n",
    "      )\n",
    ")\n",
    "\n",
    "display(silver_labs_results_final.limit(10))\n",
    "\n",
    "# --------- WRITE TO SILVER ----------\n",
    "(silver_labs_results_final\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(SILVER_LAB_RESULTS)\n",
    ")\n",
    "\n",
    "print(\"âœ… silver_lab_results written to:\", SILVER_LAB_RESULTS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48d86b6-a2b3-4012-be41-47e85cfa2d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.parquet(SILVER_LAB_RESULTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad37324b-af89-4c2e-b22f-4df26840b1f9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":30},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760461507063}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ae = spark.read.json(BRONZE_API_ADVERSE_EVENTS_PATH) \n",
    "display(ae.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458c3dc5-d837-4a64-879c-49b00cb9416a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, initcap, coalesce, to_timestamp, regexp_replace, length,\n",
    "    when, datediff, current_timestamp\n",
    ")\n",
    "\n",
    "# --------- INPUT / OUTPUT PATHS ----------\n",
    "# Make sure these are set in your notebook:\n",
    "# storage_account   = \"adlsclinicalpoc\"\n",
    "# container_raw     = \"raw\"      or \"bronze\"\n",
    "# container_silver  = \"silver\"\n",
    "BRONZE_API_ADVERSE_EVENTS_PATH       = f\"abfss://{container_bronze}@{storage_account}.dfs.core.windows.net/api_adverse_events/\"\n",
    "SILVER_ADVERSE_EVENTS      = f\"abfss://{container_silver}@{storage_account}.dfs.core.windows.net/adverse_events/\"\n",
    "\n",
    "# --------- SCHEMA (strings in, weâ€™ll parse later) ----------\n",
    "ae_schema = StructType([\n",
    "    StructField(\"event_id\",                 StringType(), True),\n",
    "    StructField(\"subject_id\",            StringType(), True),\n",
    "    StructField(\"trial_id\",          StringType(), True),\n",
    "    StructField(\"preferred_term\",                  StringType(), True),  # e.g., \"headache\"\n",
    "    StructField(\"onset_date\",    StringType(), True),  # e.g., \"2025-07-10T09:15:00Z\"\n",
    "    StructField(\"resolved_date\", StringType(), True),  # may be null\n",
    "    StructField(\"severity\",              StringType(), True),  # e.g., mild/moderate/severe\n",
    "    StructField(\"serious\",           StringType(), True),  # e.g., serious/non-serious\n",
    "    StructField(\"outcome\",               StringType(), True),  # e.g., RECOVERED, NOT RECOVERED\n",
    "    StructField(\"relatedness\",          StringType(), True),  # e.g., RELATED/NOT RELATED/UNKNOWN\n",
    "])\n",
    "\n",
    "# --------- READ BRONZE (JSON) ----------\n",
    "raw_ae = (\n",
    "    spark.read\n",
    "         .schema(ae_schema)\n",
    "         \n",
    "\n",
    "         .option(\"multiline\", \"false\")\n",
    "         .json(BRONZE_API_ADVERSE_EVENTS_PATH)\n",
    ")\n",
    "\n",
    "# --------- BASIC NORMALIZATION ----------\n",
    "ae_norm = (\n",
    "    raw_ae\n",
    "      .withColumn(\"event_id\",        trim(col(\"event_id\")))\n",
    "      .withColumn(\"subject_id\",   trim(col(\"subject_id\")))\n",
    "      .withColumn(\"trial_id\", trim(col(\"trial_id\")))\n",
    "      .withColumn(\"preferred_term\",         initcap(trim(col(\"preferred_term\"))))  # \"Headache\", \"Injection Site Pain\"\n",
    "      .withColumn(\"severity\",     upper(trim(col(\"severity\"))))\n",
    "      .withColumn(\"serious\",  upper(trim(col(\"serious\"))))\n",
    "      .withColumn(\"outcome\",      upper(trim(col(\"outcome\"))))\n",
    "      .withColumn(\"relatedness\", upper(trim(col(\"relatedness\"))))\n",
    ")\n",
    "\n",
    "# --------- CLEAN + PARSE DATETIMES (remove 'T' / 'Z' / 'UTC', try common patterns) ----------\n",
    "def parse_clean_ts(scol):\n",
    "    c = trim(regexp_replace(regexp_replace(scol, \"T\", \" \"), r\"(?i)Z|UTC$\", \"\"))\n",
    "    return coalesce(\n",
    "        to_timestamp(c, \"yyyy-MM-dd HH:mm:ss\"),\n",
    "        to_timestamp(c, \"yyyy/MM/dd HH:mm:ss\"),\n",
    "        to_timestamp(c, \"dd-MM-yyyy HH:mm:ss\"),\n",
    "        to_timestamp(c, \"dd/MM/yyyy HH:mm:ss\"),\n",
    "        to_timestamp(c, \"yyyy-MM-dd HH:mm\"),     # no seconds\n",
    "        to_timestamp(c, \"dd-MM-yyyy HH:mm\"),\n",
    "        to_timestamp(c, \"dd/MM/yyyy HH:mm\"),\n",
    "        to_timestamp(c, \"yyyy-MM-dd\"),           # date-only â†’ midnight\n",
    "        to_timestamp(c, \"dd-MM-yyyy\"),\n",
    "        to_timestamp(c, \"dd/MM/yyyy\")\n",
    "    )\n",
    "\n",
    "ae_ts = (\n",
    "    ae_norm\n",
    "      .withColumn(\"onset_date\",    parse_clean_ts(col(\"onset_date\")))\n",
    "      .withColumn(\"resolved_date\", parse_clean_ts(col(\"resolved_date\")))\n",
    ")\n",
    "\n",
    "# --------- NORMALIZE CATEGORICALS ----------\n",
    "# Severity â†’ MILD / MODERATE / SEVERE / UNKNOWN\n",
    "ae_cats = (\n",
    "    ae_ts\n",
    "      .withColumn(\n",
    "          \"severity\",\n",
    "          when(col(\"severity\").isin(\"MILD\", \"MI\", \"LOW\"), \"MILD\")\n",
    "          .when(col(\"severity\").isin(\"MODERATE\", \"MOD\", \"MEDIUM\"), \"MODERATE\")\n",
    "          .when(col(\"severity\").isin(\"SEVERE\", \"SEV\", \"HIGH\"), \"SEVERE\")\n",
    "          .otherwise(\"UNKNOWN\")\n",
    "      )\n",
    "      # Seriousness â†’ SERIOUS / NON-SERIOUS / UNKNOWN\n",
    "      .withColumn(\n",
    "          \"serious\",\n",
    "          when(col(\"serious\").isin(\"SERIOUS\", \"YES\", \"Y\"), \"SERIOUS\")\n",
    "          .when(col(\"serious\").isin(\"NON-SERIOUS\", \"NO\", \"N\", \"NOT SERIOUS\"), \"NON-SERIOUS\")\n",
    "          .otherwise(\"UNKNOWN\")\n",
    "      )\n",
    "      # Relationship â†’ RELATED / NOT RELATED / POSSIBLE / UNKNOWN\n",
    "      .withColumn(\n",
    "          \"relatedness\",\n",
    "          when(col(\"relatedness\").isin(\"RELATED\", \"CERTAIN\", \"PROBABLE\", \"LIKELY\"), \"RELATED\")\n",
    "          .when(col(\"relatedness\").isin(\"NOT RELATED\", \"UNRELATED\"), \"NOT RELATED\")\n",
    "          .when(col(\"relatedness\").isin(\"POSSIBLE\", \"POSSIBLY RELATED\"), \"POSSIBLE\")\n",
    "          .otherwise(\"UNKNOWN\")\n",
    "      )\n",
    "      # Outcome â†’ RECOVERED / RECOVERING / NOT RECOVERED / FATAL / UNKNOWN (basic map)\n",
    "      .withColumn(\n",
    "          \"outcome\",\n",
    "          when(col(\"outcome\").isin(\"RECOVERED\", \"RESOLVED\"), \"RECOVERED\")\n",
    "          .when(col(\"outcome\").isin(\"RECOVERING\", \"IMPROVING\"), \"RECOVERING\")\n",
    "          .when(col(\"outcome\").isin(\"NOT RECOVERED\", \"NOT RESOLVED\", \"PERSISTING\"), \"NOT RECOVERED\")\n",
    "          .when(col(\"outcome\").isin(\"FATAL\", \"DEATH\"), \"FATAL\")\n",
    "          .otherwise(\"UNKNOWN\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# --------- DERIVED FIELDS ----------\n",
    "ae_derived = (\n",
    "    ae_cats\n",
    "      # duration_days: if resolved is null â†’ keep null (or set to current; choose policy)\n",
    "      .withColumn(\"duration_days\",\n",
    "                  when(col(\"resolved_date\").isNotNull(),\n",
    "                       datediff(col(\"resolved_date\"), col(\"onset_date\")))\n",
    "                  .otherwise(None))\n",
    "      # flag ongoing events\n",
    "      .withColumn(\"is_ongoing\", col(\"resolved_date\").isNull())\n",
    "      # guard against negative durations (bad data: resolved before onset)\n",
    "      .withColumn(\"duration_days\",\n",
    "                  when(col(\"duration_days\") < 0, None).otherwise(col(\"duration_days\")))\n",
    ")\n",
    "# Add this code before the 'ae_filtered' step to check\n",
    "print(\"Count of records per event_id:\")\n",
    "(ae_derived\n",
    "    .groupBy(\"event_id\")\n",
    "    .count()\n",
    "    .orderBy(col(\"count\").desc())\n",
    "    .show()\n",
    ")\n",
    "# --------- ENFORCE KEYS + DEDUPE ----------\n",
    "ae_filtered = (\n",
    "    ae_derived\n",
    "      .filter((col(\"event_id\").isNotNull()) & (length(col(\"event_id\")) > 0))\n",
    "      .filter((col(\"subject_id\").isNotNull()) & (length(col(\"subject_id\")) > 0))\n",
    "      .dropDuplicates([\"event_id\"])\n",
    ")\n",
    "\n",
    "# --------- FINAL SILVER SELECT ----------\n",
    "silver_adverse_events = ae_filtered.select(\n",
    "    \"event_id\",\"subject_id\",\"trial_id\",\n",
    "    \"preferred_term\",\"severity\",\"serious\",\"relatedness\",\"outcome\",\n",
    "    \"onset_date\",\"resolved_date\",\n",
    "    \"duration_days\",\"is_ongoing\"\n",
    ")\n",
    "\n",
    "display(silver_adverse_events.limit(20))\n",
    "print(\"Null onset:\", silver_adverse_events.filter(col(\"onset_date\").isNull()).count())\n",
    "print(\"Null severity:\", silver_adverse_events.filter(col(\"severity\").isNull()).count())\n",
    "\n",
    "#--------- WRITE TO SILVER (Parquet) ----------\n",
    "(silver_adverse_events\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(SILVER_ADVERSE_EVENTS)\n",
    ")\n",
    "print(\"âœ… silver_adverse_events written to:\", SILVER_ADVERSE_EVENTS)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_layer_script (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
